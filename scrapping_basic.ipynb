{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "433fd0dc-8f6a-4a82-adbd-0a93499614d2",
   "metadata": {},
   "source": [
    "# request 패키지"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b7a9e3a4-a16c-4de3-9630-04de685a5d04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in c:\\users\\0\\anaconda3\\lib\\site-packages (2.31.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\0\\anaconda3\\lib\\site-packages (from requests) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\0\\anaconda3\\lib\\site-packages (from requests) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\0\\anaconda3\\lib\\site-packages (from requests) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\0\\anaconda3\\lib\\site-packages (from requests) (2024.2.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "69fbd65f-a515-45d7-adfe-f1f26b13e7bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "URL = 'https://www.naver.com'    # http 's' = 보안 프로토콜\n",
    "response = requests.get(URL)     # get 방식으로 요청\n",
    "print(response.status_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0e210da-e8fc-44e1-b7d1-82b03eb74585",
   "metadata": {},
   "outputs": [],
   "source": [
    "URL = 'https://search.naver.com/search.naver'\n",
    "query = {'query':'python'}\n",
    "response = requests.get(URL, params=query)\n",
    "print(response.status_code)\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3260d90d-88ee-477e-8aa5-d9d226f8380a",
   "metadata": {},
   "source": [
    "# user-agent 값 설정"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39898371-78cc-49f2-8d6a-be8c7d958b45",
   "metadata": {},
   "source": [
    "- 로봇이 아님을 나타내기 위해서 user-agent라는 값을 header에 넣어서 보냄\n",
    "- 직접적인 URL 주소로 요청 시, 웹 사이츠에서 웹  크롤링을 통해 접근한 것을 감지하고 접속을 차단하게 됨\n",
    "- user-agent 헤어값을 표함하여 요청하면 브라우저를 통해 요청하는 것으로 인식되어 해결\n",
    "- 웹 브라우저 실행 => F12 개발자 모드 진입 => Console에 nevigator.userAgent, 입력):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b080339e-eb42-44ef-9b64-91ac89dd0ef4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "저장완료!\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "URL = 'http://www.google.com/search'\n",
    "params = {'q':'%%python'}\n",
    "headers = {'user-agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/123.0.0.0 Safari/537.36'}\n",
    "\n",
    "response = requests.get(URL, params = params)  #, headers = headers)\n",
    "response.raise_for_status()  # 응답코드가 200이 아니면 오류 내고 멈춤\n",
    "\n",
    "result = response.text\n",
    "with open('mygoogle.html','w', encoding='utf-8') as f:\n",
    "    f.write(result)\n",
    "print('저장완료!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "151f871d-9734-4824-a68b-b12a512390cd",
   "metadata": {},
   "source": [
    "## [실습] 네이버 데이터랩에서 실시간 인기 검색어 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ddc13a43-18f3-4c7d-95b8-19ac541de9c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "트위드자켓\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "response = requests.get('https://datalab.naver.com')\n",
    "html_text = response.text\n",
    "\n",
    "temp = html_text.split('<em class=\"num\">1</em>')[1]\n",
    "temp = temp.split('<span class=\"title\">')[1]\n",
    "temp = temp.split('</span>')[0]\n",
    "print(temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e30fe794-5eb0-45e4-9320-c21369ed7e38",
   "metadata": {},
   "source": [
    "# BeautifulSoup 패키지"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3ad34f7-0874-447e-982e-20cf6247beee",
   "metadata": {},
   "source": [
    "## Parser별 출력 결과 비교a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e329e863-3058-495e-9c2a-e2e5abe30efa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: lxml in c:\\users\\0\\anaconda3\\lib\\site-packages (4.9.3)\n"
     ]
    }
   ],
   "source": [
    "# !pip install html5lib\n",
    "!pip install lxml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fde8c7ca-24b8-4ad2-94f5-1a4fa92ccaa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "html.parser\n",
      "<a>,</a>\n",
      "----------------------------------------\n",
      "lxml\n",
      "<html><body><a>,</a></body></html>\n",
      "----------------------------------------\n",
      "xml\n",
      "<?xml version=\"1.0\" encoding=\"utf-8\"?>\n",
      "<a/>\n",
      "----------------------------------------\n",
      "html5lib\n",
      "<html><head></head><body><a><p></p>,</a></body></html>\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "soup = BeautifulSoup('<a></p>,','html.parser')\n",
    "print('html.parser')\n",
    "print(soup)\n",
    "print('-'*40)\n",
    "\n",
    "soup = BeautifulSoup('<a></p>,','lxml')\n",
    "print('lxml')\n",
    "print(soup)\n",
    "print('-'*40)\n",
    "\n",
    "soup = BeautifulSoup('<a></p>,','xml')\n",
    "print('xml')\n",
    "print(soup)\n",
    "print('-'*40)\n",
    "\n",
    "soup = BeautifulSoup('<a></p>,','html5lib')\n",
    "print('html5lib')\n",
    "print(soup)\n",
    "print('-'*40)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de30131e-0818-4807-b9af-9428d771fecc",
   "metadata": {},
   "source": [
    "## 기본 사용법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e9cb440a-23bc-4e18-a7a3-6b3385ef50bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<title>웹 크롤러 - 위키백과, 우리 모두의 백과사전</title>\n",
      " 이 문서는 2023년 4월 30일 (일) 18:34에 마지막으로 편집되었습니다.\n",
      "<a class=\"mw-jump-link\" href=\"#bodyContent\">본문으로 이동</a>\n",
      "#bodyContent\n",
      "<a href=\"/wiki/%EA%B5%AC%EA%B8%80%EB%B4%87\" title=\"구글봇\">구글봇</a>\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "URL = 'https://ko.wikipedia.org/wiki/%EC%9B%B9_%ED%81%AC%EB%A1%A4%EB%9F%AC'\n",
    "response = requests.get(URL)\n",
    "\n",
    "# soup 객체 생성\n",
    "soup = BeautifulSoup(response.text, 'lxml')\n",
    "\n",
    "# 태그를 이용한 접근\n",
    "print(soup.title)\n",
    "print(soup.footer.ul.li.text)\n",
    "\n",
    "# 태그와 속성을 이용한 접근\n",
    "print(soup.a) # soup 객체에서 첫 번째로 만나는 a 태그 출력\n",
    "# print(soup.a['id'])  # 만약 속성이 존재하지 않으면 에러 발생\n",
    "print(soup.a['href'])   # 특정 태그 속성값\n",
    "\n",
    "# find() 함수를 이용한 태그 내의 다양한 속성을 이용한 접근\n",
    "print(soup.find('a', attrs = {'title':'구글봇'})) # a 태그 중 title 속성의 값이 '구글봇'인 데이터 검색"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "561097aa-35d6-4404-9ecf-7df25b74e00c",
   "metadata": {},
   "source": [
    "## 자식 노드들을 반복 가능한 객체로 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b51ad031-2bc1-4cad-a59a-59fea41c248d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "<p align=\"center\" class=\"a\">text1</p>\n",
      "\n",
      "\n",
      "<p align=\"center\" class=\"b\">text2</p>\n",
      "\n",
      "\n",
      "<p align=\"center\" class=\"c\">text3</p>\n",
      "\n",
      "\n",
      "<div>\n",
      "<img height=\"200\" src=\"/source\" width=\"300\"/>\n",
      "</div>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "html = '''\n",
    "<html>\n",
    "    <head>\n",
    "        <tilte>Web Scrapping</title>\n",
    "    </head>\n",
    "    <body>\n",
    "        <p class=\"a\" align=\"center\">text1</p>\n",
    "        <p class=\"b\" align=\"center\">text2</p>\n",
    "        <p class=\"c\" align=\"center\">text3</p>\n",
    "        <div>\n",
    "            <img src=\"/source\" width=\"300\" height=\"200\">\n",
    "        </div>\n",
    "    </body>\n",
    "</html>\n",
    "'''\n",
    "from bs4 import BeautifulSoup\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "contents = soup.find('body')\n",
    "# print(contents)\n",
    "for child in contents.children:\n",
    "    print(child)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "551b100b-f1e7-4497-8002-fb1a6e32d876",
   "metadata": {},
   "source": [
    "## 자신을 포함한 부모 노드까지 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dc297574-5dc3-4d5f-940d-ef4ac9fc0ca7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<img height=\"200\" src=\"/source\" width=\"300\"/>\n",
      "\n",
      "<div>\n",
      "<img height=\"200\" src=\"/source\" width=\"300\"/>\n",
      "</div>\n"
     ]
    }
   ],
   "source": [
    "html = '''\n",
    "<html>\n",
    "    <head>\n",
    "        <tilte>Web Scrapping</title>\n",
    "    </head>\n",
    "    <body>\n",
    "        <p class=\"a\" align=\"center\">text1</p>\n",
    "        <p class=\"b\" align=\"center\">text2</p>\n",
    "        <p class=\"c\" align=\"center\">text3</p>\n",
    "        <div>\n",
    "            <img src=\"/source\" width=\"300\" height=\"200\">\n",
    "        </div>\n",
    "    </body>\n",
    "</html>\n",
    "'''\n",
    "from bs4 import BeautifulSoup\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "contents = soup.find('body')\n",
    "img_tag = contents.find('img')\n",
    "print(img_tag)\n",
    "print()\n",
    "print(img_tag.parent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2e7b01f-6927-419b-a0b9-b89084e80e66",
   "metadata": {},
   "source": [
    "## 특정 부모 노드까지 검색해서 올라감"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "956e89d2-6918-49d7-9ecc-ba82cbb8b92f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<body>\n",
      "<p align=\"center\" class=\"a\">text1</p>\n",
      "<p align=\"center\" class=\"b\">text2</p>\n",
      "<p align=\"center\" class=\"c\">text3</p>\n",
      "<div>\n",
      "<img height=\"200\" src=\"/source\" width=\"300\"/>\n",
      "</div>\n",
      "</body>\n"
     ]
    }
   ],
   "source": [
    "html = '''\n",
    "<html>\n",
    "    <head>\n",
    "        <tilte>Web Scrapping</title>\n",
    "    </head>\n",
    "    <body>\n",
    "        <p class=\"a\" align=\"center\">text1</p>\n",
    "        <p class=\"b\" align=\"center\">text2</p>\n",
    "        <p class=\"c\" align=\"center\">text3</p>\n",
    "        <div>\n",
    "            <img src=\"/source\" width=\"300\" height=\"200\">\n",
    "        </div>\n",
    "    </body>\n",
    "</html>\n",
    "'''\n",
    "from bs4 import BeautifulSoup\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "contents = soup.find('body')\n",
    "\n",
    "img_tag = contents.find('img')\n",
    "print(img_tag.find_parent('body'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bff77bef-95bb-49a3-a557-2bfcbba225c8",
   "metadata": {},
   "source": [
    "## 형제 노드 검색"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3f42b1e1-bf17-4f4c-aae2-8baa97fff44a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<p align=\"center\" class=\"b\">text2</p>\n",
      "-- 바로 다음 형제 노드 --\n",
      "<p align=\"center\" class=\"c\">text3</p>\n",
      "-- 모든 다음 형제 노드 --\n",
      "[<p align=\"center\" class=\"c\">text3</p>, <div>\n",
      "<img height=\"200\" src=\"/source\" width=\"300\"/>\n",
      "</div>]\n",
      "-- 바로 이전 형제 노드 --\n",
      "<p align=\"center\" class=\"a\">text1</p>\n",
      "-- 모든 이전 형제 노드 --\n",
      "[<p align=\"center\" class=\"a\">text1</p>]\n"
     ]
    }
   ],
   "source": [
    "html = '''\n",
    "<html>\n",
    "    <head>\n",
    "        <tilte>Web Scrapping</title>\n",
    "    </head>\n",
    "    <body>\n",
    "        <p class=\"a\" align=\"center\">text1</p>\n",
    "        <p class=\"b\" align=\"center\">text2</p>\n",
    "        <p class=\"c\" align=\"center\">text3</p>\n",
    "        <div>\n",
    "            <img src=\"/source\" width=\"300\" height=\"200\">\n",
    "        </div>\n",
    "    </body>\n",
    "</html>\n",
    "'''\n",
    "from bs4 import BeautifulSoup\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "p_tag = soup.find('p', attrs={'class':'b'})\n",
    "print(p_tag)\n",
    "print('-- 바로 다음 형제 노드 --')\n",
    "print(p_tag.find_next_sibling())\n",
    "print('-- 모든 다음 형제 노드 --')\n",
    "print(p_tag.find_next_siblings())                    # 뒤에 s가 붙는 것들은 결과값 리스트로 가져옴\n",
    "print('-- 바로 이전 형제 노드 --')\n",
    "print(p_tag.find_previous_sibling())\n",
    "print('-- 모든 이전 형제 노드 --')\n",
    "print(p_tag.find_previous_siblings())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b1e40bd-7be6-4972-a414-2d68f97e285f",
   "metadata": {},
   "source": [
    "## 검색: find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a0c5c4d-cad5-4e50-b88f-f6866ca4978f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "response = requests.get('http://www.naver.com')\n",
    "soup = BeautifulSoup(response.text, 'lxml')\n",
    "print(soup.find('title'))\n",
    "print(soup.find('a'))\n",
    "print(soup.find(id='search')) # id속성의 값이 'search'인 정보 가져옴, soup.find(attrs={'id':'search'}) 동일"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec38b2d1-065f-4317-ab5d-dde2df220e7c",
   "metadata": {},
   "source": [
    "## 검색: find_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "edb0295b-ccb0-4ccd-8a71-522ad9438a43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "[<a href=\"#topAsideButton\"><span>상단영역 바로가기</span></a>, <a href=\"#shortcutArea\"><span>서비스 메뉴 바로가기</span></a>]\n"
     ]
    }
   ],
   "source": [
    "a_tags = soup.find_all('a', limit=2)\n",
    "print(len(a_tags))\n",
    "print(a_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ec6354f-25e2-4bc8-aeef-e7688acee4e5",
   "metadata": {},
   "source": [
    "[실습] 네이버 뉴스 페이지에서 언론사 목록 가져오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e10b6b80-9082-4f18-a0e2-839d8392ea27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['시사IN', '한겨레', '서울신문', '오마이뉴스', '비즈워치', '한국일보', '동아사이언스', '뉴시스', '아시아경제', 'JTBC']\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "res = requests.get('http://news.naver.com')\n",
    "soup = BeautifulSoup(res.text, 'lxml')\n",
    "result = soup.find_all('h4', attrs={'class':'channel'})\n",
    "# print(len(result))\n",
    "# print(result[0])\n",
    "# print(list(result[0].children))\n",
    "press_list = [list(tag.children)[0] for tag in result]\n",
    "print(press_list[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7ec3f0f4-4547-47dd-a1d4-2385fd5e3182",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['노컷뉴스', '오마이뉴스', '디지털타임스', '데일리안', '더팩트', '디지털데일리', '아이뉴스24', '머니S', '블로터', '프레시안']\n"
     ]
    }
   ],
   "source": [
    "res = requests.get('https://news.naver.com')\n",
    "soup = BeautifulSoup(res.text, 'lxml')\n",
    "result = soup.find_all('div', attrs = {'class':'cjs_age_name'})\n",
    "press_list = [tag.text for tag in result]\n",
    "print(press_list[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45ce3362-9a9a-4f4b-89e5-cabb30ea355d",
   "metadata": {},
   "source": [
    "## 검색: select_one(), select()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8e97dd55-f70c-4fc8-be7d-c2873f317d17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<a class=\"show\" href=\"/page/KITA_MAIN\">KITA 무역아카데미</a>\n",
      "220\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "res = requests.get('http://www.tradecampus.com')\n",
    "soup = BeautifulSoup(res.text, 'html.parser')\n",
    "\n",
    "print(soup.select_one('div > a'))\n",
    "result = soup.select('div a')\n",
    "print(len(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f4ff6d22-3bf4-43bb-89b3-9c1e17cc1f0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<h3 class=\"tit\">공지사항</h3>\n"
     ]
    }
   ],
   "source": [
    "print(soup.select_one('body > div > div.wrapper.main_page > div.renew_main > div.col-12 > div > div.renew_main_notice > div > div > h3'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c76c063c-5be4-44e4-bdae-1a97e381fb6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "제31기 수출입기업 영업이익의 보전을 위한 원스톱 환리스크관리 개강(3/21)\n"
     ]
    }
   ],
   "source": [
    "# tradecampus.com 메인 페이지 공지사항 2번째 항목 선택\n",
    "# nth-child(#)을 지우면 해당 요소(element)의 모든 요소를 가져온다.\n",
    "notice = soup.select('body > div > div.wrapper.main_page > div.renew_main > div.col-12 > div > div.renew_main_notice > div > ul > li:nth-child(2) > p > a')\n",
    "print(notice[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed321a34-f723-4f15-9e02-ac29642ea2cd",
   "metadata": {},
   "source": [
    "## 텍스트 가져오기: text, get_text()\n",
    "- 검색 결과에서 태그를 제외한 텍스트만 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "792b5349-14f2-47a7-ba19-4a06dd36d87d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "고객센터\n",
      "\n",
      "\n",
      "오프라인 교육, e러닝\n",
      "02-6000-5378/5379\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "운영시간\n",
      "평일 09:00~18:00 (주말/공휴일 : 휴무)\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 고객센터 영역 텍스트 가져오기\n",
    "tag = soup.find('div', attrs={'class':'serviceInfo'})\n",
    "# print(tag)\n",
    "print(tag.get_text())           # = tag.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "baa82a47-eefb-4df5-93d7-25d936087b63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/weven_template_repository/theme/KITAAC/1/resource/img/ico_sns_facebook.png\n",
      "/weven_template_repository/theme/KITAAC/1/resource/img/ico_sns_facebook.png\n"
     ]
    }
   ],
   "source": [
    "# 풋터에 있는 kita 로고 이미지 추출\n",
    "tag = soup.find('img', attrs = {'class':'mobile_icon black'})\n",
    "print(tag['src'])\n",
    "print(tag.get('src'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54d16573-c0cb-4759-b43e-b9cdfad3e04e",
   "metadata": {},
   "source": [
    "## 텍스트 가져오기: string\n",
    "- 검색 결과에서 **태그 안에 또 다른 태그가 없는 경우** 해당 내용을 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "eb1513eb-4876-4198-8e8a-e33408a82039",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "오프라인 교육, e러닝\n"
     ]
    }
   ],
   "source": [
    "tag = soup.find('div', attrs={'class':'serviceInfo'})\n",
    "tag = tag.find('span')\n",
    "print(tag.string)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "628b1ce5-ed64-43e4-9669-5c734bfd2d01",
   "metadata": {},
   "source": [
    "[실습] 네이버 웹툰 제목 가져오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e583f9e8-3d6b-40f9-9bb5-ed399bf174c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting selenium\n",
      "  Downloading selenium-4.19.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: urllib3<3,>=1.26 in c:\\users\\0\\anaconda3\\lib\\site-packages (from urllib3[socks]<3,>=1.26->selenium) (2.0.7)\n",
      "Collecting trio~=0.17 (from selenium)\n",
      "  Downloading trio-0.25.0-py3-none-any.whl.metadata (8.7 kB)\n",
      "Collecting trio-websocket~=0.9 (from selenium)\n",
      "  Downloading trio_websocket-0.11.1-py3-none-any.whl.metadata (4.7 kB)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in c:\\users\\0\\anaconda3\\lib\\site-packages (from selenium) (2024.2.2)\n",
      "Requirement already satisfied: typing_extensions>=4.9.0 in c:\\users\\0\\anaconda3\\lib\\site-packages (from selenium) (4.9.0)\n",
      "Collecting attrs>=23.2.0 (from trio~=0.17->selenium)\n",
      "  Downloading attrs-23.2.0-py3-none-any.whl.metadata (9.5 kB)\n",
      "Requirement already satisfied: sortedcontainers in c:\\users\\0\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (2.4.0)\n",
      "Requirement already satisfied: idna in c:\\users\\0\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (3.4)\n",
      "Collecting outcome (from trio~=0.17->selenium)\n",
      "  Downloading outcome-1.3.0.post0-py2.py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: sniffio>=1.3.0 in c:\\users\\0\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.3.0)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\users\\0\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.16.0)\n",
      "Collecting wsproto>=0.14 (from trio-websocket~=0.9->selenium)\n",
      "  Downloading wsproto-1.2.0-py3-none-any.whl.metadata (5.6 kB)\n",
      "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in c:\\users\\0\\anaconda3\\lib\\site-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
      "Requirement already satisfied: pycparser in c:\\users\\0\\anaconda3\\lib\\site-packages (from cffi>=1.14->trio~=0.17->selenium) (2.21)\n",
      "Collecting h11<1,>=0.9.0 (from wsproto>=0.14->trio-websocket~=0.9->selenium)\n",
      "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
      "Downloading selenium-4.19.0-py3-none-any.whl (10.5 MB)\n",
      "   ---------------------------------------- 0.0/10.5 MB ? eta -:--:--\n",
      "   -- ------------------------------------- 0.6/10.5 MB 13.5 MB/s eta 0:00:01\n",
      "   ------ --------------------------------- 1.7/10.5 MB 17.9 MB/s eta 0:00:01\n",
      "   --------- ------------------------------ 2.5/10.5 MB 18.0 MB/s eta 0:00:01\n",
      "   ------------- -------------------------- 3.6/10.5 MB 20.7 MB/s eta 0:00:01\n",
      "   ----------------- ---------------------- 4.7/10.5 MB 21.3 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 5.7/10.5 MB 21.4 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 6.7/10.5 MB 21.5 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 7.8/10.5 MB 21.6 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 8.8/10.5 MB 21.6 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 9.8/10.5 MB 21.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  10.5/10.5 MB 22.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 10.5/10.5 MB 21.1 MB/s eta 0:00:00\n",
      "Downloading trio-0.25.0-py3-none-any.whl (467 kB)\n",
      "   ---------------------------------------- 0.0/467.2 kB ? eta -:--:--\n",
      "   --------------------------------------- 467.2/467.2 kB 28.6 MB/s eta 0:00:00\n",
      "Downloading trio_websocket-0.11.1-py3-none-any.whl (17 kB)\n",
      "Downloading attrs-23.2.0-py3-none-any.whl (60 kB)\n",
      "   ---------------------------------------- 0.0/60.8 kB ? eta -:--:--\n",
      "   ---------------------------------------- 60.8/60.8 kB 3.2 MB/s eta 0:00:00\n",
      "Downloading wsproto-1.2.0-py3-none-any.whl (24 kB)\n",
      "Downloading outcome-1.3.0.post0-py2.py3-none-any.whl (10 kB)\n",
      "Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "   ---------------------------------------- 0.0/58.3 kB ? eta -:--:--\n",
      "   ---------------------------------------- 58.3/58.3 kB ? eta 0:00:00\n",
      "Installing collected packages: h11, attrs, wsproto, outcome, trio, trio-websocket, selenium\n",
      "  Attempting uninstall: attrs\n",
      "    Found existing installation: attrs 23.1.0\n",
      "    Uninstalling attrs-23.1.0:\n",
      "      Successfully uninstalled attrs-23.1.0\n",
      "Successfully installed attrs-23.2.0 h11-0.14.0 outcome-1.3.0.post0 selenium-4.19.0 trio-0.25.0 trio-websocket-0.11.1 wsproto-1.2.0\n"
     ]
    }
   ],
   "source": [
    "!pip install selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4aa9c78c-0118-4db9-95af-b03428ca0aea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting webdriver_manager\n",
      "  Downloading webdriver_manager-4.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: requests in c:\\users\\0\\anaconda3\\lib\\site-packages (from webdriver_manager) (2.31.0)\n",
      "Requirement already satisfied: python-dotenv in c:\\users\\0\\anaconda3\\lib\\site-packages (from webdriver_manager) (0.21.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\0\\anaconda3\\lib\\site-packages (from webdriver_manager) (23.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\0\\anaconda3\\lib\\site-packages (from requests->webdriver_manager) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\0\\anaconda3\\lib\\site-packages (from requests->webdriver_manager) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\0\\anaconda3\\lib\\site-packages (from requests->webdriver_manager) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\0\\anaconda3\\lib\\site-packages (from requests->webdriver_manager) (2024.2.2)\n",
      "Downloading webdriver_manager-4.0.1-py2.py3-none-any.whl (27 kB)\n",
      "Installing collected packages: webdriver_manager\n",
      "Successfully installed webdriver_manager-4.0.1\n"
     ]
    }
   ],
   "source": [
    "!pip install webdriver_manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6e0310e-60d9-4180-a82b-7be573a90275",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver import Chrome, ChromeOptions\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "driver = Chrome(service=Service(ChromeDriverManager().install()), options=ChromeOptions())\n",
    "\n",
    "URL = 'https://comic.naver.com/webtoon'\n",
    "driver.get(URL)\n",
    "time.sleep(4)  # 동적으로 생성되는 페이지의 내용이 완성될 때까지 대기\n",
    "soup = BeautifulSoup(driver.page_source, 'lxml')\n",
    "\n",
    "# 요일별 전체 웹툰 CSS 선택자\n",
    "temp = soup.select_one('#container > div.component_wrap.type2 > div.WeekdayMainView__daily_all_wrap--UvRFc')\n",
    "# print(temp)\n",
    "# 요일별 ul 태그 검색\n",
    "temp = temp.find_all('ul', attrs={'class':'WeekdayMainView__daily_list--R52q0'})\n",
    "week = ['월','화','수','목','금','토','일']\n",
    "for i, w in enumerate(temp):\n",
    "    print(f'----- {week[i]}요 웹툰-----')\n",
    "    webtoon_list = w.find_all('li',attrs = {'class':'DailyListItem__item--LP6_T'})\n",
    "    for webtoon in webtoon_list:\n",
    "        print(webtoon.find('span', attrs = {'class':'text'}).text)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d90e8669-1bc7-4a54-b771-33bb9bbf2234",
   "metadata": {},
   "source": [
    "[실습] 메가박스 영화정보 사이트에서 영화 포스터 다운로드 하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f0a076a-830e-4f78-a564-750efb383d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 사전 테스트: 박스 오피스 1위 영화 포스터 이미지 가져오기\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver import ChromeOptions, Chrome\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "driver = Chrome(service = Service(ChromeDriverManager().install()), options=ChromeOptions())\n",
    "URL = 'https://www.megabox.co.kr/movie'\n",
    "driver.get(URL)\n",
    "soup = BeautifulSoup(driver.page_source, 'lxml')\n",
    "poster_img = soup.select('#movieList > li > div.movie-list-info > img')\n",
    "print(len(poster_img))\n",
    "poster_img_src = poster_img[0].get('src')\n",
    "\n",
    "import requests\n",
    "res = requests.get(poster_img_src)\n",
    "with open('poster.jpg','wb') as f:\n",
    "    f.write(res.content)\n",
    "print('End')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "342ae2c6-9c80-4ef4-a09c-9f24b1565e25",
   "metadata": {},
   "source": [
    "[문제] 메가박스 영화 사이트에서 첫 페이지에 있는 모든 영화 포스트 이미지 수집하기\n",
    "- 메가박스 영화 사이트 첫 페이지에 있는 20개의 영화 포스트 이미지 수집\n",
    "- 현재 작업 디렉토리 밑에 'poster_img' 폴더가 없는 경우 폴더를 생성한다. (os 패키지 이용)\n",
    "- 저장되는 각 포스터 이미지의 파일 이름은 영화 제목으로 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5be9942-8c0a-4e75-ba1c-3f3b0cbf8f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver import ChromeOptions, Chrome\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "driver = Chrome(service = Service(ChromeDriverManager().install()), options=ChromeOptions())\n",
    "URL = 'https://www.megabox.co.kr/movie'\n",
    "driver.get(URL)\n",
    "soup = BeautifulSoup(driver.page_source, 'lxml')\n",
    "\n",
    "# 포스터 이미지를 가지고 있는 모든 img 태그를 검색\n",
    "poster_imgs = soup.find_all('img', attrs = {'class':'poster lozad'})\n",
    "\n",
    "# 이미지를 저장할 폴더 생성\n",
    "import os\n",
    "img_dir = './poster_img/'\n",
    "if not os.path.exists(img_dir):\n",
    "    os.mkdir(img_dir)\n",
    "    print('폴더 생성 완료')\n",
    "else:\n",
    "    print('폴더가 존재함')\n",
    "\n",
    "for i, poster in enumerate(poster_imgs, 1):\n",
    "    title = poster.get('alt')\n",
    "    img_url = poster.get('src')\n",
    "\n",
    "    print(i, ':', img_url)\n",
    "    img_res = requests.get(img_url)          # 이미지 재요청 => html이 아닌 이미지 (binary data) \n",
    "\n",
    "    if ':' in title:\n",
    "        title = title.replace(':','')\n",
    "\n",
    "    with open(img_dir+f'[{i}].{title}.jpg', 'wb') as f:\n",
    "        f.write(img_res.content)               # binary data이기 때문에 content로 가져옴"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6bbbdf0-15b8-4e12-9c15-dea788078b05",
   "metadata": {},
   "source": [
    "[실습] 네이버 뉴스 사이트에서 경제 관련 언론사별 랭킹뉴스 추출하기\n",
    "- 언론사 이름에 '경제' 단어가 포함된 언론사의 랭킹 뉴스만 추출하여 출력한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c5f5d00-f979-45bd-88ad-5587f0302d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "URL = 'https://news.naver.com/main/ranking/popularDay.naver'\n",
    "res = requests.get(URL)\n",
    "soup = BeautifulSoup(res.text, 'html.parser')\n",
    "\n",
    "news_list = soup.find_all('div', attrs = {'class':'rankingnews_box'})\n",
    "print('등록 언론사 개수: ', len(news_list))\n",
    "\n",
    "for news in news_list:\n",
    "    press_title = news.find('strong').text\n",
    "    if '경제' in press_title:\n",
    "        print('언론사:', press_title)\n",
    "        press_news = news.find_all('div', attrs = {'class':'list_content'})\n",
    "        for i, ranking_news in enumerate(press_news, 1):\n",
    "            print(f'{i}: {ranking_news.find(\"a\").get_text()}')\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef4b19d-5577-4739-8f58-781e0c15db3c",
   "metadata": {},
   "source": [
    "# Selenium 패키지"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b574f39-7c63-4b80-ae44-bb1f1878e85e",
   "metadata": {},
   "source": [
    "## find_element() 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24854500-d28c-4b44-8c17-52d363b1595e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium.webdriver import Chrome, ChromeOptions\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "# 웹드라이버 동적 다운로드 방식\n",
    "driver = Chrome(service=Service(ChromeDriverManager().install()), options=ChromeOptions())\n",
    "\n",
    "# [참고] 로컬 컴퓨터에 설치되어 있는 웹 드라이버 실행 방식\n",
    "# s = Service('d:\\DEV\\chromedriver\\chromedriver.exe')\n",
    "# driver = Chrome(service=s)\n",
    "\n",
    "driver.get('https://www.daum.net')\n",
    "ele = driver.find_element(by=By.LINK_TEXT, value='카페')\n",
    "print(type(ele))\n",
    "print(ele.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18d9603f-a186-44c4-aab3-a47f5d1a1611",
   "metadata": {},
   "source": [
    "## 이벤트 제어하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b39e73a-0cce-4556-911c-59c21bbb5021",
   "metadata": {},
   "source": [
    "### click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd238c79-776c-4715-a11e-c181bd821bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium.webdriver import Chrome, ChromeOptions\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "driver = Chrome(service=Service(ChromeDriverManager().install()), options=ChromeOptions())\n",
    "\n",
    "driver.get('https://www.naver.com')\n",
    "ele = driver.find_element(by=By.CSS_SELECTOR, value='#shortcutArea > ul > li:nth-child(5) > a')\n",
    "ele.click()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30c01b9b-c38b-4d2e-9d26-17a5c0879c8b",
   "metadata": {},
   "source": [
    "### send_keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9f39f51-215b-4c5b-9d44-42a6b729ec24",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium.webdriver import Chrome, ChromeOptions\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.common.keys import Keys  # 특수키 사용을 위한 클래스\n",
    "\n",
    "driver = Chrome(service=Service(ChromeDriverManager().install()), options=ChromeOptions())\n",
    "\n",
    "driver.get('https://www.naver.com')\n",
    "ele = driver.find_element(by=By.ID, value='query')\n",
    "ele.send_keys('python')\n",
    "ele.send_keys(Keys.ENTER)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c47f5095-8e46-4b77-86a1-4eaf816061f4",
   "metadata": {},
   "source": [
    "[실습] 네이버 로그인 하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f9a2c2-b105-4efc-997b-041225a01b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium.webdriver import Chrome, ChromeOptions\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.common.keys import Keys  # 특수키 사용을 위한 클래스\n",
    "\n",
    "driver = Chrome(service=Service(ChromeDriverManager().install()), options=ChromeOptions())\n",
    "\n",
    "driver.get('https://www.naver.com')\n",
    "ele = driver.find_element(by=By.CLASS_NAME, value='MyView-module__link_login___HpHMW')\n",
    "ele.click()\n",
    "my_id = 'trigger1227'\n",
    "my_pw = 'triggeris307651'\n",
    "\n",
    "# 로봇에 의해 클릭되지 못하도록 막았기 때문에 스크립트로 처리해야 함\n",
    "# ele = driver.find_element(by=By.ID, value = 'id')\n",
    "# ele.send_keys(my_id)\n",
    "# ele = driver.find_element(by=By.ID, value = 'pw')\n",
    "# ele.send_keys(my_pw)\n",
    "\n",
    "driver.execute_script(f\"document.getElementById('id').value = '{my_id}'\")\n",
    "driver.execute_script(f\"document.getElementById('pw').value = '{my_pw}'\")\n",
    "# 로그인 상태 유지 체크박스 클릭\n",
    "driver.execute_script(f'document.getElementById(\"keep\").value = \"on\"')\n",
    "\n",
    "ele = driver.find_element(by=By.CLASS_NAME, value = 'btn_login')\n",
    "ele.click()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "825ed2c3-9d00-44d4-9767-8d7e21a8c598",
   "metadata": {},
   "source": [
    "## 웹 브라우저 자동 스크롤"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76394779-858c-467c-bf83-dc852da85175",
   "metadata": {},
   "source": [
    "### 구글에서 이미지 검색 후 검색 결과 6번 스크롤 하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "173ce925-de09-4a01-9840-6c276c9fecd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium.webdriver import Chrome, ChromeOptions\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys \n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import time \n",
    "\n",
    "#페이지가 로드될 때까지 기다리는 시간\n",
    "SCROLL_PAUSE_TIME = 2 \n",
    "\n",
    "driver = Chrome(service=Service(ChromeDriverManager().install()),options=ChromeOptions())\n",
    "driver.get('https://www.google.com')\n",
    "ele=driver.find_element(by=By.CLASS_NAME,value='gLFyf')\n",
    "ele.send_keys('python')\n",
    "ele.submit()\n",
    "\n",
    "driver.find_element(By.LINK_TEXT,'이미지').click()\n",
    "\n",
    "#페이지가 로드될 때까지 기다림\n",
    "time.sleep(SCROLL_PAUSE_TIME)\n",
    "#최초 스크롤 바의 높이 값 읽기\n",
    "last_height = driver.execute_script('return document.body.scrollHeight')\n",
    "print('last height:', last_height)\n",
    "for i in range(6):\n",
    "    #윈도우의 스크롤 바를 0에서부터 가장 밑(scrollHeight)까지 이동\n",
    "    driver.execute_script('window.scrollTo(0,document.body.scrollHeight)')\n",
    "    time.sleep(SCROLL_PAUSE_TIME)\n",
    "    #스크롤 바 이동으로 새로운 검색 결과가 로딩 후 변경된 새로운 스크롤 바의 높이값 읽\n",
    "    new_height = driver.execute_script('return document.body.scrollHeight')\n",
    "    print('new height:', new_height)\n",
    "    print('-'*30)\n",
    "\n",
    "#더 이상 스크롤 될 페이지가 없을 경우 scrollHeight 값의 변화가 없다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5f29f0e-136e-4044-b590-27df8c26f208",
   "metadata": {},
   "source": [
    "### 구글에서 이미지 검색 후 검색 결과 무한 스크롤 하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "350fb86c-0679-4aed-802b-fd29a941715c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium.webdriver import Chrome, ChromeOptions\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys \n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import time \n",
    "\n",
    "#페이지가 로드될 때까지 기다리는 시간\n",
    "SCROLL_PAUSE_TIME = 2 \n",
    "\n",
    "driver = Chrome(service=Service(ChromeDriverManager().install()),options=ChromeOptions())\n",
    "driver.get('https://www.google.com')\n",
    "ele=driver.find_element(by=By.CLASS_NAME,value='gLFyf')\n",
    "ele.send_keys('python')\n",
    "ele.submit()\n",
    "\n",
    "driver.find_element(By.LINK_TEXT,'이미지').click()\n",
    "\n",
    "#페이지가 로드될 때까지 기다림\n",
    "time.sleep(SCROLL_PAUSE_TIME)\n",
    "#최초 스크롤 바의 높이 값 읽기\n",
    "last_height = driver.execute_script('return document.body.scrollHeight')\n",
    "\n",
    "# 스크롤 횟수\n",
    "scroll_cnt = 0\n",
    "\n",
    "while True:\n",
    "    #윈도우의 스크롤 바를 0에서부터 가장 밑(scrollHeight)까지 이동\n",
    "    driver.execute_script('window.scrollTo(0,document.body.scrollHeight)')\n",
    "    scroll_cnt += 1\n",
    "    time.sleep(SCROLL_PAUSE_TIME)\n",
    "    #스크롤 바 이동으로 새로운 검색 결과가 로딩 후 변경된 새로운 스크롤 바의 높이값 읽\n",
    "    new_height = driver.execute_script('return document.body.scrollHeight')\n",
    "    print(f'last height: {last_height}, new height: {new_height}, scroll count: {scroll_cnt}')\n",
    "    if last_height != new_height:  # 계속해서 new height 값이 변경되면\n",
    "        last_height = new_height\n",
    "    else:  # 더이상 new_height 값의 변동이 없다면...\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00abf37c-004d-4224-8aff-208ff954de19",
   "metadata": {},
   "source": [
    "### 구글에서 이미지 검색 후 썸네일 이미지 클릭하고 원본 이미지 주소 수집하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fcfd954-6325-4a1d-aa5d-69d300a0160b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium.webdriver import Chrome, ChromeOptions\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys \n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import time \n",
    "\n",
    "SCROLL_PAUSE_TIME = 2   #페이지가 로드될 때까지 기다리는 시간\n",
    "IMAGE_EXTRACT_NUM = 20  # 이미지 추출 개수\n",
    "SEARCH_KEYWORD = 'python'\n",
    "\n",
    "\n",
    "######## 웹 드라이버 실행 및 구글 접속 #########\n",
    "driver = Chrome(service=Service(ChromeDriverManager().install()),options=ChromeOptions())\n",
    "driver.get('https://www.google.com')\n",
    "\n",
    "######## 검색 #########\n",
    "ele=driver.find_element(by=By.CLASS_NAME,value='gLFyf')\n",
    "ele.send_keys(SEARCH_KEYWORD)\n",
    "ele.submit()\n",
    "\n",
    "######## 이미지 검색 결과 페이지 이동 #########\n",
    "driver.find_element(By.LINK_TEXT,'이미지').click()\n",
    "\n",
    "#페이지가 로드될 때까지 기다림\n",
    "time.sleep(SCROLL_PAUSE_TIME)\n",
    "\n",
    "######## 이미지 검색 결과 페이지 스크롤 #########\n",
    "#최초 스크롤 바의 높이 값 읽기\n",
    "last_height = driver.execute_script('return document.body.scrollHeight')\n",
    "\n",
    "# 스크롤 횟수\n",
    "scroll_cnt = 0\n",
    "\n",
    "while True:\n",
    "    #윈도우의 스크롤 바를 0에서부터 가장 밑(scrollHeight)까지 이동\n",
    "    driver.execute_script('window.scrollTo(0,document.body.scrollHeight)')\n",
    "    scroll_cnt += 1\n",
    "    time.sleep(SCROLL_PAUSE_TIME)\n",
    "    #스크롤 바 이동으로 새로운 검색 결과가 로딩 후 변경된 새로운 스크롤 바의 높이값 읽\n",
    "    new_height = driver.execute_script('return document.body.scrollHeight')\n",
    "    print(f'last height: {last_height}, new height: {new_height}, scroll count: {scroll_cnt}')2\n",
    "    if last_height != new_height:  # 계속해서 new height 값이 변경되면\n",
    "        last_height = new_height\n",
    "    else:  # 더이상 new_height 값의 변동이 없다면...\n",
    "        break\n",
    "\n",
    "######## 이미지 선택 및 해당 이미지 src 추출 #########\n",
    "imgs = driver.find_elements(By.CSS_SELECTOR, value = '#rso > div > div > div.wH6SXe.u32vCb > div > div > div > div.czzyk.XOEbc > h3 > a')\n",
    "print(len(imgs))\n",
    "img_src_list = []\n",
    "img_cnt = 0\n",
    "for img in imgs:\n",
    "    try:\n",
    "        img.click()\n",
    "        img_cnt += 1\n",
    "        img_src = driver.find_element(By.XPATH, value = '//*[@id=\"Sva75c\"]/div[2]/div[2]/div[2]/div[2]/c-wiz/div/div/div/div/div[3]/div[1]/a/img[1]').get_attribute('src')\n",
    "        print(f'[{img_cnt}].{img_src}')\n",
    "        img_src_list.append(img_src)\n",
    "        if img_cnt == IMAGE_EXTRACT_NUM: break\n",
    "    except: \n",
    "        img_cnt -= 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b388902b-3d82-4d6e-bad9-340ba726af98",
   "metadata": {},
   "source": [
    "### 구글에서 이미지 검색 후 파일로 저장하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "94cba649-38d7-41ee-98f2-e4982be58d9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last height: 6971, new height: 13915, scroll count: 1\n",
      "last height: 13915, new height: 20766, scroll count: 2\n",
      "last height: 20766, new height: 27307, scroll count: 3\n",
      "last height: 27307, new height: 29905, scroll count: 4\n",
      "last height: 29905, new height: 29905, scroll count: 5\n",
      "437\n",
      "[1.https://upload.wikimedia.org/wikipedia/commons/thumb/c/c3/Python-logo-notext.svg/800px-Python-logo-notext.svg.png]\n",
      "[2.https://velog.velcdn.com/images/deep-of-machine/post/3f778fa2-2b43-42b3-9233-091424be7d73/image.png]\n",
      "[3.https://i.namu.wiki/i/mxMv5lNX8m8lUwu7yTjN6eNZh8JVuI6a_chEyMRc4V9oECkhVIl7OiPiGIOllv14uDVNuwRPVco8abCPe5xOiQ.svg]\n",
      "[4.https://images.velog.io/images/pm1100tm/post/30e9dc94-96d0-41b3-9f2f-d814e839a796/python.jpg]\n",
      "[5.https://i.namu.wiki/i/pHxeJONxIv51qQsN2ac5nX3shPEmiSlKtGVATZXUE22NHGyw9v7_Aqto6aSoCU9ODz3RKtTKCEP0E0OI7TlxMQ.webp]\n",
      "[6.https://www.askedtech.com/api/kords/admin/product/image.jpg?type=org&id=20688]\n",
      "[7.https://upload.wikimedia.org/wikipedia/commons/thumb/0/0a/Python.svg/640px-Python.svg.png]\n",
      "[8.https://fineproxy.org/wp-content/uploads/2023/05/Python.jpg]\n",
      "[9.https://img1.daumcdn.net/thumb/R800x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2Fylgth%2FbtrJD4skdzV%2FBpPtZfi7QnkHzZaAr43LNk%2Fimg.png]\n",
      "[10.https://store-images.s-microsoft.com/image/apps.46763.13683872343053742.6c777826-d7ea-427a-942f-7dbfb474c121.7cd9c7f0-9cd8-4686-bd82-f7f4a18971ec?h=464]\n",
      "[11.https://www.unite.ai/wp-content/uploads/2022/04/AI-Python-Libraries.png]\n",
      "[12.https://upload.wikimedia.org/wikipedia/commons/1/10/Brooding_female_Python_molurus_bivittatus.jpg]\n",
      "[13.https://www.snugarchive.com/static/06cc3be354c9abae90b6ebc9469d7ff1/84d4b/featured-image-python-logo.png]\n",
      "[14.https://mblogthumb-phinf.pstatic.net/MjAyMjAyMTJfNSAg/MDAxNjQ0NTkzNzE5MzQ1.q5g3zqnCq2Rt1xUmpSFx2xWRQTl4VmngS8FGT7eGD0Ig.UKr_wLSCCg8PD-v8TfDddCKFIWhKoeqh5lZM09FVrsYg.PNG.sw4r/image.png?type=w800&jopt=2]\n",
      "[15.https://www.udacity.com/blog/wp-content/uploads/2020/12/Python-Tutorial_Blog-scaled.jpeg]\n",
      "[16.https://images.velog.io/images/taeil77/post/0860d033-75cf-4101-b236-1a261c8c2c8a/python.png]\n",
      "[17.https://images.techhive.com/images/article/2016/01/1200px-burmese_python_02-100637340-large.jpg?auto=webp&quality=85,70]\n",
      "[18.https://www.pcworld.com/wp-content/uploads/2023/04/python-100763894-orig-1.jpg?quality=50&strip=all]\n",
      "[19.https://ioflood.com/blog/wp-content/uploads/2023/09/Collage-of-Python-programming-aspects-syntax-libraries-Python-symbols-logo.jpg]\n",
      "[20.https://onlinedegrees.sandiego.edu/wp-content/uploads/2023/05/6-careers-you-can-get-with-python.jpg]\n"
     ]
    }
   ],
   "source": [
    "#임시\n",
    "\n",
    "from selenium.webdriver import Chrome, ChromeOptions\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys \n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import time \n",
    "\n",
    "SCROLL_PAUSE_TIME = 2   #페이지가 로드될 때까지 기다리는 시간\n",
    "IMAGE_EXTRACT_NUM = 20 #이미지 추출 개수\n",
    "SEARCH_KEYWORD = 'python'\n",
    "\n",
    "\n",
    "#############  웹 드라이버 실행 및 구글 접속  #######################\n",
    "driver = Chrome(service=Service(ChromeDriverManager().install()),options=ChromeOptions())\n",
    "driver.get('https://www.google.com')\n",
    "\n",
    "\n",
    "#############  검색  #######################\n",
    "ele=driver.find_element(by=By.CLASS_NAME,value='gLFyf')\n",
    "ele.send_keys(SEARCH_KEYWORD)\n",
    "ele.submit()\n",
    "\n",
    "#############  이미지 검색 결과 페이지 이동  #######################\n",
    "driver.find_element(By.LINK_TEXT,'이미지').click()\n",
    "\n",
    "#페이지가 로드될 때까지 기다림\n",
    "time.sleep(SCROLL_PAUSE_TIME)\n",
    "\n",
    "#############  이미지 검색결과 페이지 스크롤  #######################\n",
    "#최초 스크롤 바의 높이 값 읽기\n",
    "last_height = driver.execute_script('return document.body.scrollHeight')\n",
    "\n",
    "#스크롤 횟수\n",
    "scroll_cnt=0\n",
    "\n",
    "while True:\n",
    "    #윈도우의 스크롤 바를 0에서부터 가장 밑(scrollHeight)까지 이동\n",
    "    driver.execute_script('window.scrollTo(0,document.body.scrollHeight)')\n",
    "    scroll_cnt += 1\n",
    "    time.sleep(SCROLL_PAUSE_TIME)\n",
    "    #스크롤 바 이동으로 새로운 검색 결과가 로딩 후 변경된 새로운 스크롤 바의 높이값 읽\n",
    "    new_height = driver.execute_script('return document.body.scrollHeight')\n",
    "    print(f'last height: {last_height}, new height: {new_height}, scroll count: {scroll_cnt}')\n",
    "    if last_height != new_height: #계속해서 new_height값이 변경 되면\n",
    "        last_height = new_height\n",
    "    else:   #더 이상 new_height 값의 변동이 없다면\n",
    "        break \n",
    "\n",
    "#############  이미지 선택 및 해당 이미지 src 추출abs  #######################\n",
    "# imgs = driver.find_elements(By.CLASS_NAME, 'YQ4gaf')\n",
    "# print(len(imgs))\n",
    "imgs = driver.find_elements(By.CSS_SELECTOR, '#rso > div > div > div.wH6SXe.u32vCb > div > div > div > div.czzyk.XOEbc > h3 > a')\n",
    "print(len(imgs))\n",
    "\n",
    "img_src_list = []\n",
    "img_cnt = 0\n",
    "for img in imgs:\n",
    "    try:\n",
    "        img_cnt += 1\n",
    "        img.click()\n",
    "        time.sleep(2)\n",
    "        img_src=driver.find_element(By.CSS_SELECTOR,'#Sva75c > div.A8mJGd.NDuZHe.OGftbe-N7Eqid-H9tDt > div.LrPjRb > div.AQyBn > div.tvh9oe.BIB1wf > c-wiz > div > div > div > div > div.v6bUne > div.p7sI2.PUxBg > a > img.sFlh5c.pT0Scc.iPVvYb').get_attribute('src')\n",
    "        #해당 태그의 경로를 나타내는xpath 표시법\n",
    "        print(f'[{img_cnt}.{img_src}]')\n",
    "        img_src_list.append(img_src)\n",
    "        if img_cnt == IMAGE_EXTRACT_NUM:\n",
    "            break\n",
    "    except:\n",
    "        img_cnt -= 1\n",
    "\n",
    "#############  검색 이미지 파일로 저장  #######################\n",
    "import os\n",
    "import requests\n",
    "\n",
    "ts=time.localtime()\n",
    "path = 'c:/Temp/'\n",
    "now = '{}.{}.{}.{}.{}.{}'.format(ts.tm_year, ts.tm_mon, ts.tm_mday, ts.tm_hour, ts.tm_min, ts.tm_sec)\n",
    "\n",
    "dir=SEARCH_KEYWORD+'/'+now+'/'\n",
    "os.chdir(path)#현재의 작업 디렉토리를 바꾸는 것\n",
    "if not os.path.exists(dir):\n",
    "    os.makedirs(dir)\n",
    "\n",
    "file_no = 1\n",
    "os.chdir(path+dir)\n",
    "for url in img_src_list:\n",
    "    extension = url.split('.')[-1] #원본 이미지에서 가져온 확장자\n",
    "    ext = '' #최종적으로 사용할 이미지의 확장자\n",
    "    if extension in ['jpg','JPG','jpeg','JPEG','png','PNG','gif','GIF']:\n",
    "        ext = '.'+extension\n",
    "    else:\n",
    "        ext ='.jpg'\n",
    "\n",
    "    file_name = str(file_no)+'-'+SEARCH_KEYWORD+ext\n",
    "    file_no += 1\n",
    "    res = requests.get(url)\n",
    "    with open(file_name, 'wb') as f:\n",
    "        f.write(res.content)\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04fdba01-2d94-4f1a-a300-2a820bac716d",
   "metadata": {},
   "source": [
    "## select 태그 선택"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "d1f18221-e76b-4f8b-8044-60313a4fbb29",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium.webdriver import Chrome, ChromeOptions\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys \n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.support.ui import Select  # select form 제어 클래스\n",
    "\n",
    "URL = 'https://www.selenium.dev/selenium/web/formPage.html'\n",
    "driver = Chrome(service=Service(ChromeDriverManager().install()),options=ChromeOptions())\n",
    "driver.get(URL)\n",
    "\n",
    "ele = driver.find_element(By.NAME, 'selectomatic')\n",
    "select = Select(ele)\n",
    "\n",
    "# 인덱스를 기준으로 선택하기\n",
    "# select.select_by_index(2)\n",
    "\n",
    "# 보여지는 선택값 텍스트로 선택하기\n",
    "# select.select_by_visible_text('Four')\n",
    "\n",
    "# option 요소의 값으로 선택하기\n",
    "# select.select_by_value('four')\n",
    "\n",
    "# select 태그에 onchange 속성이 있어서 Select 클래스를 사용할 수 없을 때 (위의 3개 작동 X)\n",
    "driver.find_element(By.CSS_SELECTOR, 'option[value=\"four\"]').click()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b029c0a0-9a60-4df8-a868-0e0278e696c9",
   "metadata": {},
   "source": [
    "[실습과제] yes24에서 파이썬 도서 검색하기\n",
    "- yes24 사이트에서 파이썬 도서 검색 -> 검색 결과를 120개 선택\n",
    "- 검색 결과에서 도서 평점이 9.6 이상인 도서 제목과 가격, 평점 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "771cd07d-3100-421a-8df9-8557a0ed644e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium.webdriver import Chrome, ChromeOptions\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys \n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.support.ui import Select  # select form 제어 클래스\n",
    "import time\n",
    "\n",
    "URL = 'https://www.yes24.com'\n",
    "driver = Chrome(service=Service(ChromeDriverManager().install()),options=ChromeOptions())\n",
    "driver.get(URL)\n",
    "\n",
    "ele = driver.find_element(by=By.ID, value='query')\n",
    "ele.send_keys('파이썬')\n",
    "ele.send_keys(Keys.ENTER)\n",
    "\n",
    "driver.find_element(By.CSS_SELECTOR, '#stat_gb > option[value=\"120\"]').click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ab082420-549d-4b75-a204-93df76ff58aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "검색 도서 권수:  524\n",
      "001 | Do it! 점프 투 파이썬 | 19,800원 | 9.8\n",
      "002 | 실무로 통하는 인과추론 with 파이썬 | 34,200원 | 9.8\n",
      "003 | 혼자 공부하는 머신러닝+딥러닝 | 23,400원 | 9.6\n",
      "004 | 챗GPT API를 활용한 챗봇 만들기 | 28,800원 | 10.0\n",
      "005 | 코딩 자율학습 나도코딩의 파이썬 입문 | 21,600원 | 9.8\n",
      "006 | Hey, 파이썬! 생성형 AI 활용 앱 만들어 줘 | 35,100원 | 9.8\n",
      "007 | 챗GPT로 만드는 주식 & 암호화폐 자동매매 시스템 | 18,000원 | 9.8\n",
      "008 | 혼자 공부하는 데이터 분석 with 파이썬 | 23,400원 | 9.7\n",
      "009 | 마이크로 파이썬을 활용해 사물인터넷(IoT) 프로젝트 만들기 with ESP32 | 20,700원 | 10.0\n",
      "010 | 새내기 파이썬 | 30,000원 | 10.0\n",
      "011 | ChatGPT 소스를 얹는 파이썬 레시피 | 28,800원 | 10.0\n",
      "012 | 으뜸 파이썬 | 32,000원 | 9.6\n",
      "013 | 코딩 입문자를 위한 문제해결 기반 파이썬 4.0 | 27,000원 | 10.0\n",
      "014 | 파이썬으로 쉽게 배우는 자료구조 | 29,000원 | 10.0\n",
      "015 | 시간순삭 파이썬 | 22,500원 | 10.0\n",
      "016 | 데이터 과학을 위한 기초수학 with 파이썬 | 23,400원 | 10.0\n",
      "017 | Do it! 점프 투 파이썬 | 15,000원 | 9.8\n",
      "018 | 데이터 과학을 위한 파이썬 프로그래밍 | 30,000원 | 10.0\n",
      "019 | 자료구조와 알고리즘 with 파이썬 | 21,600원 | 9.8\n",
      "020 | 코딩 테스트 합격자 되기 - 파이썬 편 | 36,000원 | 9.9\n",
      "021 | 파이썬을 이용한 퀀트 투자 포트폴리오 만들기 | 27,000원 | 10.0\n",
      "022 | 데이터 과학 기반의 파이썬 빅데이터 분석 | 28,000원 | 10.0\n",
      "023 | Do it! 쉽게 배우는 파이썬 데이터 분석 | 20,700원 | 9.8\n",
      "024 | 파이썬 데이터 분석 & 시각화 + 웹 대시보드 제작하기 | 15,930원 | 9.7\n",
      "025 | 난생처음 데이터 분석 with 파이썬 | 26,000원 | 10.0\n",
      "026 | Do it! 게임 10개 만들며 배우는 파이썬 | 19,800원 | 10.0\n",
      "027 | Do it! 첫 파이썬 | 12,600원 | 9.8\n",
      "028 | 나는 파이썬으로 머신러닝한다 1 | 19,800원 | 10.0\n",
      "029 | Do it! 첫 코딩 with 파이썬 | 16,200원 | 9.9\n",
      "030 | 혼자 공부하는 머신러닝+딥러닝 | 20,800원 | 9.6\n",
      "031 | 모두의 파이썬 | 10,800원 | 9.6\n",
      "032 | 컴퓨팅 사고 with 파이썬 | 25,000원 | 10.0\n",
      "033 | 파이썬 코딩 도장 | 27,000원 | 9.8\n",
      "034 | 컴퓨팅 사고와 파이썬 | 24,000원 | 10.0\n",
      "035 | 파이썬 라이브러리를 활용한 머신러닝 | 29,700원 | 9.9\n",
      "036 | 소프트웨어 코딩 대회를 위한 파이썬 문제 풀이 100 | 18,000원 | 9.8\n",
      "037 | 혼자 공부하는 데이터 분석 with 파이썬 | 20,800원 | 9.7\n",
      "038 | 나는 파이썬으로 머신러닝한다 2 | 20,700원 | 10.0\n",
      "039 | 모두의 인공지능 with 파이썬 | 19,800원 | 10.0\n",
      "040 | 실무로 통하는 인과추론 with 파이썬 | 30,400원 | 9.8\n",
      "041 | 파이썬 기초 문법 | 0원 | 10.0\n",
      "042 | 챗GPT API를 활용한 챗봇 만들기 | 25,600원 | 10.0\n",
      "043 | 머신 러닝 교과서 with 파이썬, 사이킷런, 텐서플로 | 39,600원 | 10.0\n",
      "044 | 챗GPT로 만드는 주식 & 암호화폐 자동매매 시스템 | 14,000원 | 9.8\n",
      "045 | 밑바닥부터 시작하는 딥러닝 2 | 26,100원 | 9.8\n",
      "046 | 쉽게 따라 만드는 파이썬 주식 자동매매 시스템 | 25,200원 | 9.6\n",
      "047 | 컴퓨터 비전과 딥러닝 | 39,000원 | 9.6\n",
      "048 | 컴퓨터 사이언스 부트캠프 with 파이썬 | 21,600원 | 10.0\n",
      "049 | 사장님 몰래하는 파이썬 업무 자동화 | 28,800원 | 9.7\n",
      "050 | 창의적 문제 해결을 위한 파이썬 프로그래밍 | 23,000원 | 10.0\n",
      "051 | 머신 러닝·딥 러닝에 필요한 기초 수학 with 파이썬 | 24,300원 | 10.0\n",
      "052 | 파이썬 텍스트 마이닝 완벽 가이드 | 27,000원 | 10.0\n",
      "053 | 파이썬으로 경험하는 빅데이터 분석과 머신러닝 | 27,000원 | 10.0\n",
      "054 | 파이썬으로 배우는 소프트웨어와 인공지능 | 25,000원 | 10.0\n"
     ]
    }
   ],
   "source": [
    "# 데이터 가져오기\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "soup = BeautifulSoup(driver.page_source, 'lxml')\n",
    "book_list = soup.find('ul', attrs = {'id':'yesSchList'})\n",
    "books = book_list.find_all('li')\n",
    "count = 0\n",
    "print('검색 도서 권수: ', len(books))\n",
    "for book in books:\n",
    "    rating = book.find('span', attrs = {'class':'rating_grade'})\n",
    "    if not rating: continue  # 평점이 없는 도서일 경우, 이하 내용 실행 skip\n",
    "    rating = float(rating.find('em').get_text())\n",
    "    if rating >= 9.6:\n",
    "        count += 1\n",
    "        title = book.find('a', attrs = {'class':'gd_name'}).get_text()\n",
    "        price = book.find('strong', attrs = {'class':'txt_num'}).get_text()\n",
    "        print(f'{count:03d} | {title} | {price} | {rating}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66ca8a6f-bbaa-4564-a47d-2334b6fc84bd",
   "metadata": {},
   "source": [
    "[실습] 메가박스 영화 감상평 및 평점 수집"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0f7255df-3529-4c4b-bd98-09884fbc5934",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium.webdriver import Chrome, ChromeOptions\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys \n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "\n",
    "PAUSE_TIME = 3\n",
    "\n",
    "URL = 'https://www.megabox.co.kr/movie'\n",
    "driver = Chrome(service=Service(ChromeDriverManager().install()),options=ChromeOptions())\n",
    "driver.get(URL)\n",
    "\n",
    "movie = driver.find_element(By.CSS_SELECTOR, '#movieList > li:nth-child(4) > div.movie-list-info > div.movie-score > a').send_keys(Keys.ENTER)\n",
    "time.sleep(PAUSE_TIME)\n",
    "driver.find_element(By.LINK_TEXT, '실관람평').click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d9b0019b-2215-4138-a50b-eb2381c59428",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 페이지 수:  2555\n",
      "1페이지 평점 및 리뷰 정보 수집\n",
      "2페이지 평점 및 리뷰 정보 수집\n",
      "3페이지 평점 및 리뷰 정보 수집\n",
      "4페이지 평점 및 리뷰 정보 수집\n",
      "5페이지 평점 및 리뷰 정보 수집\n",
      "6페이지 평점 및 리뷰 정보 수집\n",
      "7페이지 평점 및 리뷰 정보 수집\n",
      "8페이지 평점 및 리뷰 정보 수집\n",
      "9페이지 평점 및 리뷰 정보 수집\n",
      "10페이지 평점 및 리뷰 정보 수집\n",
      "11페이지 평점 및 리뷰 정보 수집\n",
      "12페이지 평점 및 리뷰 정보 수집\n",
      "13페이지 평점 및 리뷰 정보 수집\n",
      "14페이지 평점 및 리뷰 정보 수집\n",
      "15페이지 평점 및 리뷰 정보 수집\n",
      "16페이지 평점 및 리뷰 정보 수집\n",
      "17페이지 평점 및 리뷰 정보 수집\n",
      "18페이지 평점 및 리뷰 정보 수집\n",
      "19페이지 평점 및 리뷰 정보 수집\n",
      "20페이지 평점 및 리뷰 정보 수집\n"
     ]
    }
   ],
   "source": [
    "# 페이지 번호 클릭 테스트\n",
    "# 마지막 페이지 이동\n",
    "driver.find_element(By.CSS_SELECTOR, '#contentData > div > div.movie-idv-story > nav > a.control.last').click()\n",
    "time.sleep(PAUSE_TIME)\n",
    "total_page_num = int(driver.find_element(By.CSS_SELECTOR, '#contentData > div > div.movie-idv-story > nav > strong').text)\n",
    "print('전체 페이지 수: ', total_page_num)\n",
    "# 첫 페이지 이동\n",
    "driver.find_element(By.CSS_SELECTOR, '#contentData > div > div.movie-idv-story > nav > a.control.first').click()\n",
    "time.sleep(PAUSE_TIME)\n",
    "\n",
    "ratings = []\n",
    "comments = []\n",
    "\n",
    "THE_LAST_PAGE = 20\n",
    "next_a_tag = 2  # 다음 클릭이 이루어질 a 태그 순서 (첫 페이지는 a 태그 아닌 strong 태그라서, 첫 클릭이 이뤄질 태그 순서는 2부터 시작)\n",
    "\n",
    "for i in range(1, total_page_num+1):\n",
    "    if i > THE_LAST_PAGE: break\n",
    "    print(f'{i}페이지 평점 및 리뷰 정보 수집')\n",
    "    bs = BeautifulSoup(driver.page_source, 'lxml')\n",
    "    result = bs.find_all('li', attrs = {'class':'type01 oneContentTag'})\n",
    "    for c in result:\n",
    "        rating = int(c.find('div', attrs = {'class':'story-point'}).text)\n",
    "        comment = c.find('div', attrs = {'class':'story-txt'}).text.strip()\n",
    "        ratings.append(rating)\n",
    "        comments.append(comment)\n",
    "\n",
    "    if not i%10:  # 10번째 페이지 평점 정보 수집이 끝나면...\n",
    "        driver.find_element(By.CSS_SELECTOR, '#contentData > div > div.movie-idv-story > nav > a.control.next').click()  # 다음 10페이지 보기\n",
    "        time.sleep(PAUSE_TIME)\n",
    "        next_a_tag = 4  # 다음 10페이지에서 그 다음 클릭이 이뤄질 a 태그의 순서는 4가 됨 (다음 클릭할 페이지 링크 앞에 <, <<, 첫페이지가 있기 때문)\n",
    "        continue\n",
    "        \n",
    "    # 다음 페이지 번호 클릭\n",
    "    driver.find_element(By.CSS_SELECTOR, f'#contentData > div > div.movie-idv-story > nav > a:nth-child({next_a_tag})').click()\n",
    "    next_a_tag += 1\n",
    "    time.sleep(PAUSE_TIME)\n",
    "\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "97f0297c-618d-4941-a3fb-a0661e475e7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "관람객 평점:9.2점\n",
      "평점 저장 완료\n",
      "감상평 저장 완료\n"
     ]
    }
   ],
   "source": [
    "print(f'관람객 평점:{sum(ratings)/len(ratings):.1f}점')\n",
    "\n",
    "import csv\n",
    "import os\n",
    "\n",
    "with open('ratings.csv', 'w') as f:\n",
    "    writer = csv.writer(f)\n",
    "    for rating in ratings:\n",
    "        writer.writerow([rating])   # 행단위로 평점 입력 (행단위 = 리스트 형태 필요)\n",
    "print('평점 저장 완료')\n",
    "\n",
    "with open('comments.txt', 'w', encoding='utf-8') as f:\n",
    "    for comment in comments:\n",
    "        f.write(comment+'\\n')\n",
    "print('감상평 저장 완료')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
